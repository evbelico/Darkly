[ Walkthrough ]

Most websites host a 'robots.txt' file at their root. It mostly helps in SEO purposes, by telling web browsers which parts of your website they can and can't index (it is useful if you have a private members section on your website for example, and a public section. You probably don't want to index the pages from the private one). So, bingo, there is actually a robots.txt file : http://192.168.99.100/robots.txt
Output :
User-agent: *
Disallow: /whatever
Disallow: /.hidden

We know there are two folders here. Let's look at /.hidden/ : http://[IP]/.hidden/
At the bottom of every folder we can find a README text file. It's nearly impossible to navigate through the folder structure without getting lost. We can find more READMEs who say various things, for example :
Non ce n'est toujours pas bon ...
Demande à ton voisin du dessous

We'll have to crawl/scrape every folder/subfolders looking for different messages in the README text files. It can be done using the tool wget (which allows to retrieve content from a webserver, installed via homebrew here), like so : wget -r -np -e robots=off -R "index.html*" http://[IP]/.hidden/ (-r for recursive, -np so it doesn't follow links to parent directory, -e robots=off to ignore robots.txt file, -R as --reject to reject all pages such as index.html&page=something).
The command takes a while, but we can then access a copy of the .hidden/ folder inside the folder with the website IP address : [IP]/.hidden/
We then create an exclusion list so we don't have to loop through all the common README messages, like :
Demande à ton voisin de gauche
Demande à ton voisin de droite
Demande à ton voisin du dessous
Demande à ton voisin du dessus
Tu veux de l'aide ? Moi aussi !
Non ce n'est toujours pas bon ...
Toujours pas tu vas craquer non ?

From the [IP]/.hidden/ folder, we can then launch : grep -R -v -f ../../exclude_list.txt * 'README' (-R for recursive, -v for term exclusion, -f for file, in which with have a list of terms to ignore, '*' for every folder, 'README' because we only want to access README files).
We get : whtccjokayshttvxycsvykxcfm/igeemtxnvexvxezqwntmzjltkt/lmpanswobhwcozdqixbowvbrhw/README:99dde1d35d1fdd283924d84e6d9f1d820
This is the flag.

How to prevent : a basic .htaccess file at the root of the folder with a 'Deny from all' rule should do the trick. 
